{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28f415b3",
   "metadata": {},
   "source": [
    "Data creation for MDD-DS style questionnaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dea2055",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"\")  \n",
    "\n",
    "df=df[df['eventname']=='baseline_year_1_arm_1']\n",
    "\n",
    "past = ['ksads_1_1_p', 'ksads_1_5_p', 'ksads_1_159_p', 'ksads_1_161_p', 'ksads_1_181_p', 'ksads_1_177_p', 'ksads_1_179_p', 'ksads_1_183_p', \n",
    "        'ksads_1_163_p', 'ksads_1_3_p', 'ksads_23_951_p', 'ksads_23_945_p', 'ksads_23_949_p', 'ksads_23_948_p', 'ksads_23_947_p', 'ksads_23_950_p', \n",
    "        'ksads_23_946_p', 'ksads_23_954_p']\n",
    "\n",
    "current = ['ksads_1_2_p', 'ksads_1_6_p', 'ksads_1_160_p', 'ksads_1_162_p', 'ksads_1_182_p', 'ksads_1_178_p', 'ksads_1_180_p', 'ksads_1_184_p', \n",
    "           'ksads_1_164_p', 'ksads_1_4_p', 'ksads_23_962_p', 'ksads_23_956_p', 'ksads_23_960_p', 'ksads_23_959_p', 'ksads_23_958_p', \n",
    "           'ksads_23_961_p', 'ksads_23_957_p', 'ksads_23_965_p']\n",
    "\n",
    "sleep_past = ['ksads_1_157_p', 'ksads_22_141_p']\n",
    "sleep_current = ['ksads_1_158_p', 'ksads_1_156_p']\n",
    "\n",
    "weight_past = ['ksads_1_171_p', 'ksads_1_167_p']\n",
    "weight_current = ['ksads_1_172_p', 'ksads_1_168_p']\n",
    "\n",
    "app_past = ['ksads_1_165_p', 'ksads_1_169_p']\n",
    "app_current = ['ksads_1_166_p', 'ksads_1_170_p']\n",
    "\n",
    "psych_past = ['ksads_1_173_p', 'ksads_1_175_p']\n",
    "psych_current = ['ksads_1_174_p', 'ksads_1_176_p']\n",
    "\n",
    "harm_past = ['ksads_23_953_p', 'ksads_23_952_p']\n",
    "harm_current = ['ksads_23_964_p', 'ksads_23_963_p']\n",
    "\n",
    "# Count occurrences of '1' in each category\n",
    "df['count_past'] = (df[past] == 1.0).sum(axis=1)\n",
    "df['count_current'] = (df[current] == 1.0).sum(axis=1)\n",
    "\n",
    "df['count_sleep'] = (df[sleep_past] == 1.0).any(axis=1).astype(int)\n",
    "df['count_weight'] = (df[weight_past] == 1.0).any(axis=1).astype(int)\n",
    "df['count_app'] = (df[app_past] == 1.0).any(axis=1).astype(int)\n",
    "df['count_psych'] = (df[psych_past] == 1.0).any(axis=1).astype(int)\n",
    "df['count_harm'] = (df[harm_past] == 1.0).any(axis=1).astype(int)\n",
    "\n",
    "df['count_sleep1'] = (df[sleep_current] == 1.0).any(axis=1).astype(int)\n",
    "df['count_weight1'] = (df[weight_current] == 1.0).any(axis=1).astype(int)\n",
    "df['count_app1'] = (df[app_current] == 1.0).any(axis=1).astype(int)\n",
    "df['count_psych1'] = (df[psych_current] == 1.0).any(axis=1).astype(int)\n",
    "df['count_harm1'] = (df[harm_current] == 1.0).any(axis=1).astype(int)\n",
    "\n",
    "df['past_score'] = df['count_past'] + df[['count_sleep', 'count_weight', 'count_app', 'count_psych', 'count_harm']].sum(axis=1)\n",
    "df['current_score'] = df['count_current'] + df[['count_sleep1', 'count_weight1', 'count_app1', 'count_psych1', 'count_harm1']].sum(axis=1)\n",
    "\n",
    "df['lifetime_past_current'] = (df[past + current] == 1.0).sum(axis=1)\n",
    "df['lifetime_sleep'] = (df[sleep_past + sleep_current] == 1.0).any(axis=1).astype(int)\n",
    "df['lifetime_weight'] = (df[weight_past + weight_current] == 1.0).any(axis=1).astype(int)\n",
    "df['lifetime_app'] = (df[app_past + app_current] == 1.0).any(axis=1).astype(int)\n",
    "df['lifetime_psych'] = (df[psych_past + psych_current] == 1.0).any(axis=1).astype(int)\n",
    "df['lifetime_harm'] = (df[harm_past + harm_current] == 1.0).any(axis=1).astype(int)\n",
    "\n",
    "# Compute final lifetime score as the sum of all lifetime indicators\n",
    "df['lifetime_score'] = (\n",
    "    df['lifetime_past_current'] + df['lifetime_sleep'] + df['lifetime_weight'] +\n",
    "    df['lifetime_app'] + df['lifetime_psych'] + df['lifetime_harm']\n",
    ")\n",
    "\n",
    "df.to_csv('data_with_scores.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c3c7a0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# QC data\n",
    "df = pd.read_csv(\"\")  \n",
    "df1=df[df['eventname']=='baseline_year_1_arm_1']\n",
    "qc_df=df1[['src_subject_id','imgincl_t1w_include']]\n",
    "print(qc_df.shape)\n",
    "\n",
    "#CBCL data\n",
    "df = pd.read_csv(\"/\")  \n",
    "df1=df[df['eventname']=='baseline_year_1_arm_1']\n",
    "cbcl_df=df1[['src_subject_id','cbcl_scr_syn_anxdep_r']]\n",
    "print(cbcl_df.shape)\n",
    "\n",
    "#Gender data\n",
    "df = pd.read_csv(\"\")\n",
    "df1=df[df['eventname']=='baseline_year_1_arm_1']\n",
    "gender_df=df1[['src_subject_id','demo_sex_v2']]\n",
    "print(gender_df.shape)\n",
    "\n",
    "#Site data\n",
    "df = pd.read_csv(\"\")\n",
    "df1=df[df['eventname']=='baseline_year_1_arm_1']\n",
    "site_df=df1[['src_subject_id','site_id_l']]\n",
    "print(site_df.shape)\n",
    "\n",
    "\n",
    "merged_df1=pd.merge(qc_df,cbcl_df,on='src_subject_id',how='inner')\n",
    "merged_df2=pd.merge(gender_df,site_df,on='src_subject_id',how='inner')\n",
    "merged_df=pd.merge(merged_df1,merged_df2,on='src_subject_id',how='inner')\n",
    "\n",
    "df1 = pd.read_csv('data_with_scores.csv')\n",
    "print(df1.shape)\n",
    "\n",
    "final_df=pd.merge(merged_df,df1,on='src_subject_id',how='inner')\n",
    "final_df['src_subject_id']=final_df['src_subject_id'].str.replace('_','')\n",
    "\n",
    "final_df.shape\n",
    "\n",
    "final_df.to_csv('final_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8951eb03",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"final_data.csv\")  \n",
    "df=df[df['imgincl_t1w_include']==1]\n",
    "df=df[:1000]\n",
    "\n",
    "images=[]\n",
    "for _,s in df.iterrows():\n",
    "    d=s['src_subject_id']\n",
    "    t=s['scan_session']\n",
    "    path=datapath+d+timepoint+t\n",
    "    file_path=os.path.join(path,'smwc1pT1.nii')\n",
    "    images.append(nib.load(file_path).get_fdata()) \n",
    "stacked_image = np.stack(images, axis=-1) \n",
    "\n",
    "print(\"Stacked Image shape \",stacked_image.shape)\n",
    "\n",
    "d = df.iloc[0]['src_subject_id']\n",
    "t = df.iloc[0]['scan_session']\n",
    "path=datapath+d+timepoint+t\n",
    "first_img = nib.load(os.path.join(path,'smwc1pT1.nii')) # load the first image to get the header\n",
    "stacked_nii_img = nib.Nifti1Image(stacked_image, first_img.affine, header=first_img.header)\n",
    "\n",
    "print(\"Stacked Image shape \",stacked_nii_img.shape)\n",
    "\n",
    "#Generating mean image and save it\n",
    "mean_img = image.mean_img(stacked_nii_img)\n",
    "print(\"Mean Image shape \",mean_img.shape)  \n",
    "\n",
    "\n",
    "from scipy.stats import pearsonr\n",
    "mean_data = mean_img.get_fdata().flatten()\n",
    "\n",
    "# Compute correlation for each image\n",
    "images=[]\n",
    "meta_data=[]\n",
    "for _,s in df.iterrows():\n",
    "    d=s['src_subject_id']\n",
    "    t=s['scan_session']\n",
    "    path=datapath+d+timepoint+t\n",
    "    file_path=os.path.join(path,'smwc1pT1.nii')\n",
    "    img=nib.load(file_path).get_fdata()\n",
    "    img_data = img.flatten()  # Flatten current image\n",
    "    corr, _ = pearsonr(img_data, mean_data)  # Compute Pearson correlation\n",
    "    if corr>0.85:\n",
    "        images.append(nib.load(file_path).get_fdata())\n",
    "        meta_data.append(s)\n",
    "print(\"Corr images len \",len(images))\n",
    "print(\"Metadata len \",len(meta_data))\n",
    "\n",
    "corr_stacked_image = np.stack(images, axis=-1) \n",
    "print(\"Corr images shape \",corr_stacked_image.shape)\n",
    "d = df.iloc[0]['src_subject_id']\n",
    "t = df.iloc[0]['scan_session']\n",
    "path=datapath+d+timepoint+t\n",
    "first_img = nib.load(os.path.join(path,'smwc1pT1.nii')) # load the first image to get the header\n",
    "corr_stacked_nii_img = nib.Nifti1Image(corr_stacked_image, first_img.affine, header=first_img.header)\n",
    "#nib.save(corr_stacked_nii_img,'Images/corr_stacked_image.nii')\n",
    "print(\"Stacked image shape \",corr_stacked_nii_img.shape)\n",
    "\n",
    "smri_data=corr_stacked_nii_img.get_fdata()\n",
    "\n",
    "print(\"Loaded data shape\", smri_data.shape)\n",
    "\n",
    "average_map = np.mean(smri_data, axis=-1)\n",
    "d = df.iloc[0]['src_subject_id']\n",
    "t = df.iloc[0]['scan_session']\n",
    "path=datapath+d+timepoint+t\n",
    "first_img = nib.load(os.path.join(path,'smwc1pT1.nii')) # load the first image to get the header\n",
    "average_img = nib.Nifti1Image(average_map,first_img.affine,header=first_img.header)\n",
    "nib.save(average_img, 'average_map_mdd_ds.nii')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0504e6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"final_data.csv\")  \n",
    "df=df[df['imgincl_t1w_include']==1]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb686644",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np \n",
    "from nibabel.testing import data_path\n",
    "import nibabel as nib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from nilearn import image\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "datapath=''\n",
    "\n",
    "SubIDs=os.listdir(datapath)\n",
    "\n",
    "timepoint=\"/Baseline/\"\n",
    "\n",
    "mean_img=image.mean_img(\"average_map_mdd_ds.nii\")\n",
    "mean_data = mean_img.get_fdata().flatten()\n",
    "\n",
    "# Compute correlation for each image\n",
    "images=[]\n",
    "meta_data=[]\n",
    "\n",
    "for _,s in df.iterrows():\n",
    "    d=s['src_subject_id']\n",
    "    t=s['scan_session']\n",
    "    path=datapath+d+timepoint+t\n",
    "    file_path=os.path.join(path,'smwc1pT1.nii')\n",
    "    img=nib.load(file_path).get_fdata()\n",
    "    img_data = img.flatten()  # Flatten current image\n",
    "    corr, _ = pearsonr(img_data, mean_data)  # Compute Pearson correlation\n",
    "    if corr>0.85:\n",
    "        images.append(nib.load(file_path).get_fdata())\n",
    "        meta_data.append(s)\n",
    "print(len(images))\n",
    "print(len(meta_data))\n",
    "\n",
    "filename='Corr_Data_mdd_ds.csv'\n",
    "meta_data_df=pd.DataFrame(meta_data)\n",
    "meta_data_df.to_csv(filename)\n",
    "meta_data_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8fca442",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from nibabel.testing import data_path\n",
    "import nibabel as nib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from nilearn import image\n",
    "datapath=''\n",
    "timepoint=\"/Baseline/\"\n",
    "meta_data= pd.read_csv(\"Corr_Data_mdd_ds.csv\")  \n",
    "images=[]\n",
    "for _,s in meta_data.iterrows():\n",
    "    d=s['src_subject_id']\n",
    "    t=s['scan_session']\n",
    "    path=datapath+d+timepoint+t\n",
    "    file_path=os.path.join(path,'smwc1pT1.nii')\n",
    "    images.append(nib.load(file_path).get_fdata()) \n",
    "stacked_image = np.stack(images, axis=-1) \n",
    "\n",
    "print(stacked_image.shape)\n",
    "\n",
    "path=datapath+d+timepoint+t\n",
    "first_img = nib.load(os.path.join(path,'smwc1pT1.nii')) \n",
    "stacked_img = nib.Nifti1Image(stacked_image,first_img.affine,header=first_img.header)\n",
    "nib.save(stacked_img, 'brain_images_mdd_ds.nii')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9239362a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import matplotlib.pyplot as plt\n",
    "from nilearn import plotting\n",
    "\n",
    "img = nib.load(\"average_map_mdd_ds.nii\")  \n",
    "data = img.get_fdata()\n",
    "plotting.view_img(img)\n",
    "\n",
    "threshold = 0.2\n",
    "mask = (data > threshold).astype(np.uint8)  # 1 where data > 0.2, else 0\n",
    "\n",
    "mask_nifti = nib.Nifti1Image(mask, img.affine, img.header)\n",
    "nib.save(mask_nifti, \"mask_mdd_ds.nii\")\n",
    "plotting.view_img(mask_nifti)\n",
    "\n",
    "# Visualize the middle slice\n",
    "slice_idx = data.shape[-1] // 2  \n",
    "plt.imshow(mask[:, :, slice_idx], cmap=\"gray\")\n",
    "plt.title(\"Binary Mask (Threshold > 0.2)\")\n",
    "plt.colorbar()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88eeab67",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nibabel as nib\n",
    "from neuroCombat import neuroCombat\n",
    "\n",
    "# Load the 4D NIfTI image (X, Y, Z, num_subjects)\n",
    "nii_img = nib.load(\"brain_images_mdd_ds.nii\")\n",
    "images_4d = nii_img.get_fdata()\n",
    "X, Y, Z, num_subjects = images_4d.shape\n",
    "num_voxels = X * Y * Z\n",
    "\n",
    "# Reshape to 2D (voxels, subjects)\n",
    "images_2d = images_4d.reshape(-1, num_subjects).astype(np.float32)\n",
    "\n",
    "# Load metadata\n",
    "meta_data_df = pd.read_csv(\"Corr_Data_mdd_ds.csv\")\n",
    "site_info = meta_data_df[\"site_id_l\"].values.astype(str)\n",
    "\n",
    "assert len(site_info) == num_subjects, \"Mismatch: site_info and number of subjects\"\n",
    "\n",
    "# Create DataFrame for neuroCombat\n",
    "covars = pd.DataFrame({'site': site_info.astype(str)})  \n",
    "\n",
    "# Convert data type to float32 for compatibility\n",
    "images_2d = images_2d.astype(np.float32)\n",
    "\n",
    "# Apply neuroCombat for harmonization\n",
    "harmonized_data = neuroCombat(dat=images_2d, covars=covars, batch_col='site')\n",
    "\n",
    "# Convert back to 4D NIfTI image\n",
    "harmonized_4d = harmonized_data['data'].reshape(X, Y, Z, num_subjects)\n",
    "\n",
    "# Save the harmonized image \n",
    "harmonized_img = nib.Nifti1Image(harmonized_4d, affine=nii_img.affine, header=nii_img.header)\n",
    "nib.save(harmonized_img, \"harmonized_brain_images_mdd_ds.nii\")\n",
    "print(\"Shape: \",harmonized_img.shape)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11628c06",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from nilearn.masking import apply_mask\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "\n",
    "mask_img=nib.load('mask_mdd_ds.nii')\n",
    "stacked_image=nib.load('harmonized_brain_images_mdd_ds.nii')\n",
    "print(stacked_image.shape)\n",
    "masked_data = apply_mask(stacked_image, mask_img)\n",
    "\n",
    "np.save(\"masked_data_mdd_ds.npy\", masked_data)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
